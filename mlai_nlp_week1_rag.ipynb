{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqflLB5ObpP_"
   },
   "source": [
    "# understanding embeddings: retrieval augmented generation demo\n",
    "## rutgers ieee ml/ai workshop by mehek üêô\n",
    "\n",
    "week 1 of natural language processing track\n",
    "\n",
    "february 11, 2026\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKhRB7N-Sqzd"
   },
   "source": [
    "# install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLjsd4RpER6d"
   },
   "outputs": [],
   "source": [
    "!pip install faiss-cpu pypdf sentence-transformers openai -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ivc-0rL5SvNp"
   },
   "source": [
    "# openai key + load sentence transformer model from hugging face\n",
    "\n",
    "if you want to rerun this notebook, you will need your own openai api key. this will be used for the chatbot interaction\n",
    "\n",
    "for the embeddings of our pdf chunks, we are able to use any embedding model we want to since we will be doing the retrieving / storing through a vector database ourselves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170,
     "referenced_widgets": [
      "7e83bb955e6e48d280a479775c568a14",
      "29d99b07c13344d5b3cbdcffbc8dece5",
      "3cca1bac791e4a76ab00241b7dc25adb",
      "c963f73634994266be863c4f4b75a024",
      "a5365da960134569ad310b795c6675e0",
      "7091a6d5d6664d1fb3c6d5d366b5b2c3",
      "4b0ea0f8b8cd4711b27c78145658c67b",
      "85bf507a52454584af929b38ec080c1e",
      "58c2e72f6f324ed0a82cfff91b629e81",
      "2fe92bf65a014509be06da018b60fc68",
      "d7119da6b8ae46fea34a2d08abe17403"
     ]
    },
    "id": "avB0ppTUHUiM",
    "outputId": "3f4fc0cf-6827-47d7-eff3-3979a7948d50"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "import faiss\n",
    "import numpy as np\n",
    "from pypdf import PdfReader\n",
    "from google.colab import files\n",
    "\n",
    "client = OpenAI(api_key=\"INSERT-API-KEY-HERE\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # fast model to load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJppx7K4TMan"
   },
   "source": [
    "# extract text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "2GKFWSrIIpjH",
    "outputId": "1b7b5b13-b2a6-4714-97a8-c114d9dfea82"
   },
   "outputs": [],
   "source": [
    "uploaded = files.upload()\n",
    "pdf_path = list(uploaded.keys())[0]\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text() + \"\\n\"\n",
    "\n",
    "print(\"Loaded PDF:\", pdf_path)\n",
    "print(\"Total characters:\", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nselbKQoTRjj"
   },
   "source": [
    "# chunk pdf into pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t59hxQnaIyA8",
    "outputId": "36fbed8e-7ad2-4264-a34b-756f8a63c7b3"
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "  # try changing the chunk_size !\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phKxA7aFTtCZ"
   },
   "source": [
    "# create embeddings for pdf chunks\n",
    "since there is only one pdf that has been loaded, the number of embedding vectors that will be stored in the database is equivalent to the number of chunks we have\n",
    "\n",
    "(1 chunk --> encoded into 1 embedding vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J3VOW6VbVQx5",
    "outputId": "f079e369-b775-4298-99c0-a0876ddd9639"
   },
   "outputs": [],
   "source": [
    "embeddings = model.encode(chunks)\n",
    "print(\"Number of embedding vectors:\", embeddings.shape)\n",
    "dimension = embeddings.shape[1]\n",
    "print(\"Embedding dimension:\", dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Vz7kPXLVF6j"
   },
   "source": [
    "# create vector database & store embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6jjDGLyVpH1"
   },
   "source": [
    "FAISS = Facebook AI Similarity Search\n",
    "- open source library for finding similarities between high-dimensional vectors\n",
    "- returns k nearest neighbor\n",
    "\n",
    "IndexFlat2 measures L2 (Euclidean) distance between all given points of a query vector and the vectors loaded into the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t089CZM6I1kW",
    "outputId": "0923e0d9-31c9-4440-d850-b4fb4e5f6119"
   },
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings).astype(\"float32\"))\n",
    "print(\"FAISS index size:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QE_CXz7fX-SX"
   },
   "source": [
    "# we can visualize our vector database\n",
    "we have to use a method called PCA to represent the database as in a 2D format since the vectors are very high dimensional (384 dimension embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "l3QdMRCIYPBt",
    "outputId": "ee7b2b89-751f-4888-be22-8448271363e6"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# convert FAISS index to numpy\n",
    "faiss_vectors = index.reconstruct_n(0, index.ntotal)\n",
    "\n",
    "# reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "points_2d = pca.fit_transform(faiss_vectors)\n",
    "\n",
    "# plot the points yayyy\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(points_2d[:,0], points_2d[:,1])\n",
    "plt.title(\"Vector Database (FAISS) - PCA Projection\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wqwkqS4YWUJ"
   },
   "source": [
    "# retrieval function\n",
    "we encode the query into an embedding vector and do a search for vectors in the database that are most similar to the query embedding vectors. we return the PDF chunks of text that correspond to the most similar k number of vectors.\n",
    "\n",
    "*we have to use the same exact embedding model that we used for the PDF chunks to be able to search in the vector database for information that is similar/relevant to the query*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0n-BMlAlI2RZ"
   },
   "outputs": [],
   "source": [
    "def retrieve(query, k=3):\n",
    "    q_emb = model.encode([query])\n",
    "    distances, indices = index.search(np.array(q_emb).astype(\"float32\"), k)\n",
    "    return [chunks[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgU2xBvEI3-P"
   },
   "outputs": [],
   "source": [
    "def answer_question_with_RAG(query):\n",
    "    retrieved = retrieve(query, k=3)\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant. Use ONLY the context below to answer.\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    If the answer is not in the context, say \"Not found in the document.\"\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    print(\"üîç Retrieved Chunks:\\n\")\n",
    "    for c in retrieved:\n",
    "        print(\"----\\n\", c[:300], \"...\\n\")\n",
    "\n",
    "    print(\"\\nü§ñ Answer:\\n\")\n",
    "    print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41yC_abQauuA"
   },
   "outputs": [],
   "source": [
    "def answer_question_without_RAG(query):\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant. Use ONLY the context below to answer.\n",
    "\n",
    "    Question: {query}\"\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    print(\"ü§ñ Answer:\\n\")\n",
    "    print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhX2cC05Zx_t"
   },
   "source": [
    "# try a query!\n",
    "we can see which PDF chunks were relevant to the query and provide that information to the chatbot (ChatGPT) along with the original query text.\n",
    "\n",
    "the final answer is what the chatbot outputs after having the extra knowledge that we provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBfstkNfI89S",
    "outputId": "038554ab-a812-452c-eaf7-0b11afddeb76"
   },
   "outputs": [],
   "source": [
    "answer_question_with_RAG(\"What are the main topics covered?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGe-LBGsbEQS"
   },
   "source": [
    "# we can look visualize the results of the retrieval\n",
    "again, we have to use a technique called PCA to represent the database in a 2D format since the raw vectors are very high dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "Pjka_ZEdJ8F4",
    "outputId": "1c040647-90bc-4e07-fcd9-dbb1c49ac020"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_retrieval(query, k=3):\n",
    "    # embed query\n",
    "    q_vec = model.encode([query]).astype(\"float32\")\n",
    "\n",
    "    # FAISS search\n",
    "    distances, indices = index.search(q_vec, k)\n",
    "    neighbors = indices[0]\n",
    "\n",
    "    # reconstruct stored vectors\n",
    "    faiss_vectors = index.reconstruct_n(0, index.ntotal)\n",
    "\n",
    "    # stack query vector with stored vectors\n",
    "    all_vecs = np.vstack([faiss_vectors, q_vec])\n",
    "\n",
    "    # reduce to 2D\n",
    "    pca = PCA(n_components=2)\n",
    "    all_2d = pca.fit_transform(all_vecs)\n",
    "    vecs_2d = all_2d[:-1]\n",
    "    query_2d = all_2d[-1]\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "\n",
    "    # plot all vectors (from the database)\n",
    "    plt.scatter(vecs_2d[:,0], vecs_2d[:,1], label=\"Document Chunks\")\n",
    "\n",
    "    # highlight retrieved ones\n",
    "    plt.scatter(vecs_2d[neighbors,0], vecs_2d[neighbors,1],\n",
    "                s=120, edgecolor='black', label=\"Retrieved Neighbors\")\n",
    "\n",
    "    # plot the query vector :D\n",
    "    plt.scatter(query_2d[0], query_2d[1],\n",
    "                s=200, marker=\"*\", label=\"Query Vector\")\n",
    "\n",
    "    plt.title(\"FAISS Retrieval Visualization (PCA Projection)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_retrieval(\"What are the main topics?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oj_u1NribhhX"
   },
   "source": [
    "# query without performing RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2k4ZH20a6DV",
    "outputId": "c914e58e-ee89-450f-cfd5-5af3f4378f7d"
   },
   "outputs": [],
   "source": [
    "answer_question_without_RAG(\"What are the main topics covered?\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
